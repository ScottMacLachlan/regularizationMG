


In this paper, we are interested in designing efficient solvers for the sequence of least regularized least squares problems
\begin{equation}  \min_{x} \| K x - b \|_2^2 + \lambda^2 \| R^{(i)} x \|_2^2 \end{equation}
or equivalently
\[ \min_x \left\| \left[ \begin{matrix} K \\ \lambda R^{(i)} \end{matrix} \right] x - \left[ \begin{matrix} b \\ 0 \end{matrix} \right] \right\|_2 \]
or 
\[ (K^TK + \lambda^2 L^T (W^{(i)})^2 L ) x = K^T b \]
where 
\[ R^{(i)} = W^{(i)} L \] 
for $L$ the discrete gradient, and $W^{(i)}$ is a diagonal weighting, and we asume the ``best'' value of the regularization parameter for this $i$th problem, $\lambda^{(i)}_{*}$, is {\bf not} known a priori.   Such problems
frequently arise in situations where the weights would ideally be computed from the desired $x_{true}$, but as this is unknown the weights are recomputed adaptively from increasingly better estimates of the desired solution.    
In our numerical examples, we consider two types of weightings from recent literature:  one which is designed to give solutions with ``sparse'' gradients \cite{IRLS} and one which is designed to preserve edges \cite{KilmerEtalICIAM15}.  

We will consider only
one well-known heuristic for estimating $\lambda^{(i)}_*$ -- the L-curve criterion -- but our solvers would also be applicable in scenarios where other selection mechanisms are used.       
The L-curve \cite{OlearyHansen,etc} is a parametric plot of $( \log_10 \|K x_{\lambda} - b \|_2,\log_10 \| R^{(i)} x_{\lambda} \|_2)$ as a function of $\lambda$.   The general idea is that the point of maximum positive curvature occurs where there is a good balance between the effect of the regularization term and data-fidelity term.   (Forward reference to a plot in the paper).   

The obvious approach is to ``solve'' on of the equivalent forms of (\ref{eq:sequence}) for a few choices of $\lambda$ and then plot the curve.   If implemented naively, however, this is expensive because
   \begin{itemize}
      \item When $\lambda$ is small,   $\left[ \begin{matrix} K \\ \lambda R^{(i)} \end{matrix} \right] $ is likely to be ill-conditioned, as the emphasis is on the ill-conditioned top block.   Thus, many iterations are required for an {\it accurate} solution.
      \item When $\lambda$ is large, the emphasis is on the bottom block, but as $i$ increases, $R^{(i)}$ tends
      to be ill-conditioned due to more accurate diagonal weighting and near zero terms.
      \end{itemize}
      

Alternatively, hybrid iterative methods for the class of regularization operators we consider ($R^{(i)}$ rectangular with possible non-trivial null-space) we consider, in which $x$ is constrained to live in a k-dimensional subspace whose basis is generated by an iterative procedure for (\ref{eq:sequence}), have been proposed \cite{KilmerEspanolHansen,KilmerEtalICIAM15}.  Related methods, not immediately applicable for the case of non-rectangular $K$ and $L$ we consider here, are given in \cite{SilvaNagy15}.   Hybrid
methods have an advantage that, given a suitable $k$, the value of $\lambda_*^{(i)}$ can be chosen for the k-dimensional projected problem very cheaply, and only one subspace is needed/created for all values of $\lambda$.   The cost of the hybrid methods varies by algorithm, but matrix vector products with $K, R^{(i)}$ and their transposes are a necessary evil, and thus the minimum cost of any of these methods is $k$ times the sum of the costs for those products.   (MEK:  need to clean this up.  emphasize non-square A, L, means must use my appraoch, but and potential expense of our hybrid method and the fact other hybrids won't work unless you could transform to standard form, then you need A-weighted pseudoinverse which is very expensive per iteration.) 

Our goal in this paper is to modify the ``obvious'' approach so that it is provides a computationally viable algorithm.
Ingredients:
   \begin{itemize}
     \item AMG preconditioning when $\lambda$ is larger and favors the diffusion like term.   
     \item Non-exact solves with $\lambda$ small (possibly combined with preconditioner for $A^TA$?  ), just to know enough about the l-curve to find the corner
     \item Leveraging properites of the l-curves as functions of $i$ to prune $\lambda$
     \end{itemize} 

 In \cite{Gazzola_etal_2020}, an inner-outer iteration scheme is
 developed for the solution of \eqref{eq:sequence}, where the diagonal
 weighting matrix is updated at each outer iteration, and a hybrid
 method is used to find both the optimal regularization parameter,
 $\lambda$, and solution of the resulting least-squares problem.  This
 hybrid algorithm is based on joint bidiagonalization (cf.,
 \cite{Kilmer_Hanson_Espanol_2007}), projecting the large
 least-squares problem into a manageable subspace where $\lambda$ can
 be selected by standard approaches (e.g., the discrepancy principle
 or an L-curve criterion) and the minimizers of the projected problem
 can be readily computed directly.  However, each step of the hybrid
 iterative regularization requires two calls to another iterative
 routine to determine a projection, and therefore each step of the
 hybrid iterative regularization can be expensive.  Here, we consider
 the same outer iteration as \cite{Gazzola_etal_2020}, but we replace
 the single hybrid iterative routine with a new, two-pronged technique
 aimed at improving the overall efficiency of the edge-preserving
 algorithm.  First, we introduce an effective multigrid preconditioner
 for the solution of the global least-squares problem for fixed
 $\lambda$.  Secondly, we make use of the fact that we solve a
 sequence of least-squares problems for different values of $\lambda$
 to introduce improved initial guesses for the solution of each
 problem, improving the overall efficiency of the
 multigrid-preconditioned solves.  Finally, in this setting, we make
 use of an observed relationship between the optimal regularization
 parameters selected for each outer iteration to reduce the search
 space for regularization parameters within the inner iteration.
