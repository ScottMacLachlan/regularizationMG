
For each outermost iteration, over $M^{(\ell)}$, we must solve the
\textit{sequence} of linear systems in \eqref{eq:normal} for a set of
values, $\lambda^{(i)}$.  As a consequence of the non-stationary
relaxation procedure described above, we use Flexible GMRES
(FGMRES) \cite{YSaad_2003a} preconditioned by the multigrid V-cycle to
solve each linear system.  To do this most efficiently, we cycle
through the chosen values of $\lambda^{(i)}$ from largest to smallest,
using the solution of the next-largest regularization parameter as the
initial guess for each system.  We choose this order because the
multigrid solver is expected to be most effective for large values of
$\lambda^{(i)}$, where the regularization term dominates.  For the
largest value of $\lambda^{(i)}$, we use the solution for the chosen
$\lambda_{\ell-1}$ from the previous iteration as the initial guess,
with a zero initial guess used for the first outer iteration.  Note
that, since the solution is expected to vary continuously with
$\lambda^{(i)}$, this generally provides very good initial guesses, at
least as we vary the regularization parameter within each outer
iteration.  This motivates the use of a stopping tolerance that is not
relative to the initial residual; here, we require the $\ell_2$ norm
of the residual to be less than a factor of $10^{-4}$ times the norm
of $K^Tb$.

{\bf TODO: More to say here? Emphasize (somewhere?) that we only
  construction 1 AMG hierarchy per outer iteration?}

Even with the efficient multigrid algorithm and improved initial
guesses, there is still a nontrivial cost to solving each linear
system in \eqref{eq:normal} for each value of $\lambda^{(i)}$.  In
order to further control the costs of the outer iterations, we now
introduce an algorithm to effectively ``trim'' the set of values
$\left\{\lambda^{(i)}\right\}$ that needs to be actively considered at
each iteration.  Recall that we solve the system for multiple values
of the parameter so that we can construct the L-curve for the
regularized problem for each choice of $M^{(\ell)}$.  As will be
observed in Section \ref{sec:numerical}, the values selected as
$\lambda_\ell$, corresponding to the ``corner'' of the L-curve,
monotonically increase with $\ell$, typically starting from the
smallest values of the set $\left\{\lambda^{(i)}\right\}$ at the first
iteration, and increasing by just one or two increments in each outer
iteration.

Rather than solving \eqref{eq:normal} for all possible values of
$\lambda^{(i)}$ at each iteration, to improve efficiency of the
algorithm, we limit ourselves to only a subset of these values.
This trimming must, clearly, be done carefully, in order to not lose
the ability to detect the corner of the L-curve from having too few
data points, or only having data points on one ``leg'' of the curve.
To do this, we consider the values $\lambda^{(i)}$ to be ordered
increasingly from smallest (denoted $\lambda^{(1)}$) to largest (with
all values assumed to be strictly positive).  For each outer
iteration, $\ell$, we keep track of the index, $i_{\ell-1}$, of the
regularization parameter chosen at iteration $\ell-1$ (with $i_0$
taken to be $1$).  Then, at iteration $\ell$, we consider only a subset
of $\left\{\lambda^{(i)}\right\}$ of size 10, ``centred'' at index
$i_{\ell-1}$.  In general (when $i_{\ell-1}$ is not too close to
either endpoint of the interval), we select those values with index
satisfying $i_{\ell-1}-2 \leq i \leq i_{\ell-1}+7$, consistent with
the expectation that $i_{\ell-1} \leq i_\ell$, but that some data for
$i \leq i_{\ell-1}$ may be needed in order to accurately pick the
corner of the L-curve.  When $i_{\ell-1}$ is close to either endpoint,
the interval is simply shifted to the smallest or largest 10 values in
$\left\{\lambda^{(i)}\right\}$.  In Section \ref{sec:numerical}, we
will compare this approach to the brute-force approach of computing
all values of $x^{i,\ell}$, showing that this can save significant
computational time without sacrificing any accuracy in the computed
solution.

