\documentclass[review]{siamart190516}
\usepackage{amsmath,amsfonts,color,ifpdf}
\usepackage{algorithm,algorithmic}
\usepackage{epsfig, graphicx}
\usepackage{gensymb}
\usepackage{cite}

\newcommand{\btrue}{b_{\mathrm{true}}}
\newcommand{\xtrue}{x_{\mathrm{true}}}

\title{Multigrid preconditioning for regularized least-squares problems}
\author{Matthias Bolten \and Misha E. Kilmer \and Scott P. MacLachlan}

\begin{document}
\maketitle

\begin{abstract}
For many inverse problems, regularization is a key step in ensuring
fidelity of the recovered solution and overcoming noisy data or
uncertain forward models. For imaging problems, in particular,
classical regularization based on the L2 norm of the solution gradient
is well-known to be a poor choice, as it fails to preserve natural
edges in the recovered solution, and so minimization based on the L1
norm or total variation is generally preferred. In this talk, we
consider solution of the sequence of linear systems that arise when
such a regularized problem is solved using a reweighed least-squares
approach to resolve the regularization term. Particular attention is
paid to the selection of components of a multigrid preconditioner for
different ranges of the regularization parameter value.  {\bf Pasted
  from Copper Mountain abstract - needs more work!}
\end{abstract}


\section{Introduction}
\input{intro}

\section{Problem Formulation}
\input{formulation}

\section{Improving Efficiency of Inner-Outer Iterations}
\input{efficiency}

\subsection{Multigrid}\label{sec:multigrid}
\input{multigrid}

\subsection{Improving Efficiency of Outer Iterations}
\input{outer}


\section{Numerical Results}\label{sec:numerical}
\input{numerical_results}

\section{Conclusions}

\bibliographystyle{siam} \bibliography{regularized}


\end{document}
