Multigrid methods (see, for
example, \cite{WLBriggs_VEHenson_SFMcCormick_2000a, UTrottenberg_etal_2001a}) are optimal solvers for systems arising from the
discretization of elliptic PDEs. Geometric multigrid methods make use
of the following observation: When an iterative method like (weighted)
Jacobi or Gauss-Seidel is applied to a linear system
\[
  K x = b, K \in \mathbb{R}^{n \times n}, x, b \in \mathbb{R}^n,
\]
arising from the discretization of a simple elliptic PDE, the
reduction of the error is relatively slow and depends on the
discretization parameter, $h$. Nevertheless, the error $e = K^{-1} b -
x$ is smooth after only a few iterations of the iterative
method. Obviously, a smooth error can be represented well on a coarser
mesh. Using the definition of the residual $r = b - K x$ we can obtain
the error as the solution of $K e = r$. If coarse representations,
$K_c$ of $K$ and $r_c$ of $r$, are available, then the system $K_c e_c = r_c$
can be solved instead and the solution is less expensive. The
restriction of $r$ to $r_c$ can be obtained by simple injection or by
more complicated schemes such as, e.g., full-weighting. The coarse-grid
operator, $K_c$, can be obtained using a rediscretization of the
continuous operator on the coarser mesh, or by using a variational
principle resulting in the Galerkin operator. The error, $e_c$, on the
coarse mesh is then interpolated to the fine one, yielding an
approximation $\tilde{e}$. Finally, the current approximate solution
can be updated as $x = x + \tilde{e}$. As the interpolation process
introduces high frequency error components in the approximate
solution, in most cases a couple of additional relaxation steps are
applied. The resulting method is called a
\emph{two-grid} method.

This idea can be applied recursively on each level, with level
$k$ approximating the solution of the coarser system, $K_{k+1} e_{k+1} = r_{k+1}$,
using $\gamma$ iterations of the described method on
level $k+1$, resulting in a multigrid method. Given relaxation
schemes on level $k$ specified by update operators,
$\mathcal{S}_k$ and $\tilde{\mathcal{S}}_k$,
$k = 0,\dots,k_\text{max}$, number of pre- and post-relaxation steps, $\nu_1$ and $\nu_2$, and restriction and prolongation operators,
$R_k$ and $P_k$, $k = 1,\dots,k_{\text{max}-1}$, a generic
multigrid algorithm can be found in Algorithm~\ref{alg:mg}.

\begin{algorithm}
  \caption{Multigrid cycle $x_k = \mathcal{MG}_k(x_k,b_k)$}
  \label{alg:mg}
  \begin{algorithmic}
    \STATE{$x_k \leftarrow
      \mathcal{S}_k^{\nu_1}(x_k,b_k)$}
    \STATE{$r_k \leftarrow b_k - K_k
      x_k$}
    \STATE{$r_{k+1} \leftarrow R_k r_k$}
    \STATE{$e_{k+1} \leftarrow 0$}
    \IF{$k+1 = k_\text{max}$}
    \STATE{$e_{k_\text{max}} \leftarrow
      K_{k_\text{max}}^{-1} r_{k_\text{max}}$}
  \ELSE
%  \FOR{$j = 1,\dots,\xi$}
  \STATE{For $i=1,\ldots,\gamma$, $e_{k+1} \leftarrow
    \mathcal{MG}_{k+1}(e_{k+1},r_{k+1})$}
%  \ENDFOR
  \ENDIF
  \STATE{$e_k \leftarrow P_k e_{k+1}$}
  \STATE{$x_k \leftarrow x_k +
    e_k$}
  \STATE{$x_k \leftarrow
    \tilde{\mathcal{S}}_k^{\nu_2}(x_k,b_k)$}
  \end{algorithmic}
\end{algorithm}

In case that the coarsening is not easily determined geometrically, an
algebraic multigrid method (AMG) can be
used \cite{ABrandt_SFMcCormick_JWRuge_1984a, JWRuge_KStuben_1987a,
KStuben_2001a}. For a given relaxation scheme, classical AMG
automatically selects coarse mesh points and the necessary restriction
and prolongation operators based on certain heuristics that are
typically derived based on M-matrix or similar properties. AMG for
symmetric problems usually chooses $R_k = P_k^T$, the coarse-grid
operator is chosen to be the Galerkin coarse-grid operator given by
$K_{k+1} = P_k^T K_k P_k$.  Once this ``setup phase'' is complete, the
AMG solution phase proceeds exactly as in the geometric MG setting, as
described in Algorithm \ref{alg:mg}.

In what follows, we make use of components of the classical
(Ruge-St\"uben) AMG algorithm as described
in \cite{JWRuge_KStuben_1987a} and elsewhere, but within a somewhat
nonstandard setup and solve phase, which are adapted to the problem
setting at hand.  Given a matrix, $K$, the coarse-grid is selected
using a two-pass algorithm that partitions the degrees of freedom in
$K$ into two disjoint sets, denoted as C and F.  First, a maximal
independent set of the graph of the so-called strong connections in
matrix $K$ is formed.  This amounts to first filtering the matrix $K$
to remove entries where
\[
-k_{ij} \leq \theta \max_{m\neq i} \{-k_{im}\},
\]
where we take $\theta = 0.25$, as is commonly done.  The maximal
independent set is then formed by a greedy algorithm that sequentially
identifies the unsorted point with largest measure (initialized as
number of strong connections), marks this as a C-point, then marks its
as-yet unsorted strong neighbours as F-points and increments the
measure of the unsorted strong neighbours of the F-points to make
these more attractive for subsequent selection as C-points.  Given
this tentative partitioning, a second-pass is performed, where
additional points are moved from the F-set to the C-set, in order to
satisfy heuristic measures that indicate an effective interpolation
operator can be found.  Once the second pass has finalized the
partitioning, the interpolation operator, $P$, is formed.  For each
point in C, we assign a coarse-grid index to the point, and
interpolate directly from the coarse-grid index to its corresponding
point in C.  For interpolation to points in F, we make the assumption
that we are interpolating errors that vary smoothly between strongly
connected points, leading to an interpolation formula for each $i\in
F$ of $e_i = \sum_{j\in C_i} w_{ij}e_j$, where $C_i$ is the set of
points in C that are strongly connected to $i$.  The interpolation
weights are given by
\[
w_{ij} = - \frac{k_{ij} + \displaystyle\sum_{m \in
    F_i^s}\frac{k_{im}k_{mj}}{\sum_{l \in C_i} k_{ml}}}{k_{ii} +
    \displaystyle\sum_{m \in F_i^w} k_{im}},
\]
where the set $F_i^s$ is the set of points in F that are strongly
connected to $i$, while the set $F_i^w$ is defined so that all points
adjacent to $i$ are contained in the disjoint union $C_i \cup
F_i^s \cup F_i^w$.

\subsection{AMG for the regularized linear systems}

To clarify notation, we now rewrite \eqref{eq:sequence} to explictly
introduce separate indices for the regularization parameter and outer
iteration, defining $x^{(i,\ell)}$ as the vector that minimizes
\[
\| A x - b \|_2^2 + \left(\lambda^{(i)}\right)^2 \| M^{(\ell)} x \|_2^2.
\]
Notice that an equivalent formulation for $x^{(i,\ell)}$ is as the
minimizer of
\begin{equation}
\label{eq:block-ls}
\left\| \left[\begin{array}{c} A \\ \lambda^{(i)}
                           M^{(\ell)}\end{array}\right]x - \left[\begin{array}{c} b \\ 0 \end{array}\right]\right\|^2,
\end{equation}
for which the normal equations are readily given by
\begin{equation}
\label{eq:normal}
\left(A^TA + \left(\lambda^{(i)}\right)^2 \left(M^{(\ell)}\right)^TM^{(\ell)}\right)x^{(i,\ell)} = A^Tb.
\end{equation}
The first matrix in \eqref{eq:normal} corresponds to the ``data'' term
from the inverse problem, which is often an integral operator or
non-differential operator that is difficult to treat directly using
AMG due to its spectral properties and expected large number of
nonzero entries per row.  In contrast, the second matrix
in \eqref{eq:normal},
\[
\left(M^{(\ell)}\right)^TM^{(\ell)} = L^T\left(D^{(\ell)}\right)^2L,
\]
represents a variable-coefficient diffusion operator, for which AMG
techniques are expected to provide quite effective and efficient
solvers.

As a first observation, we note that for large-enough values of
$\lambda^{(i)}$, the system will be dominated by its second component.
This motivates the design of an AMG-based algorithm to
solve \eqref{eq:normal}.  However, directly applying AMG to $A^TA
+ \left(\lambda^{(i)}\right)^2 L^T\left(D^{(\ell)}\right)^2L$ is
unlikely to be effective, due to the cost of applying the underlying
graph algorithms to the much denser matrix coming from $A^TA$.  Here,
we propose an alternative approach, that takes advantage of the better
sparsity of the rectangular operator in \eqref{eq:block-ls} over the
normal operator in \eqref{eq:normal}.  To define the coarse-grid
problem, we first form the second operator in \eqref{eq:normal},
$L^T\left(D^{(\ell)}\right)^2L$, and apply the classical AMG algorithm
discussed above (and in \cite{JWRuge_KStuben_1987a}, for example) to
this operator to define a hierarchy of interpolation operators, $P_k$,
for $1 \leq k \leq k_{\text{max}}-1$.  Note that, on the finest grid,
this term corresponds to a discretized differential operator of the
form for which AMG is well-known to be effective, so no modifications
to the classical AMG heuristics are needed.  Note also that this
directly calculates the Galerkin coarse-grid operators,
$P_1^TL^T\left(D^{(\ell)}\right)^2LP_1$ and so forth, as the
coarsening algorithm proceeds.

To aid in the implementation of the multigrid V-cycle, we store
several additional pieces of data within the AMG setup phase.  First,
while the AMG setup naturally computes the hierarchy of Galerkin
coarse-grid operators, we additionally compute and store the hierarchy
of ``one-sided'' coarsening operators generated by defining $A_1 = A$,
and $A_k = A_{k-1}P_k$ as well as $M_1 = M^{(\ell)} = D^{(\ell)}L$,
and $M_k = M_{k-1}P_k$.  Note that storage of these two hierarchies of
operators gives the necessary information to compute matrix-vector
products with the full Galerkin coarsening of the normal equations
in \eqref{eq:normal} on any level of the grid hierarchy , since
$P_1^TA^TAP_1 = (AP_1)^T(AP_1)$, for example, but without the
(possibly significant) expense of forming and storing $A^TA$ (and its
Galerkin coarsenings).  Note, also, the important connection between
the one-sided coarsening hierarchies and the optimal coarse-grid
correction from the least-squares formulation, which would be to solve
\[
\min_{y_c}\left\| \left[\begin{array}{c} A \\ 
      \lambda^{(\ell)}D^{(\ell)}L\end{array}\right](\hat{x}+Py_c) - \left[\begin{array}{c} b \\ 0 \end{array}\right]\right\|^2,
\]
for a given approximation, $\hat{x}$, to $x^{(i,\ell)}$.  As the
interpolation and coarse-grid operators do not depend on
$\lambda^{(i)}$, these terms can be formed once for each outer
iteration (corresponding to each choice of $M^{(\ell)}$) and reused
for all values of $\lambda^{(i)}$ for which a multigrid iteration is
needed.  To aid in the efficient implementation of a weighted-Jacobi
style relaxation scheme on each level, we also compute and store
vectors corresponding to the diagonals of $A_k^TA_K$ and $M_k^TM_k$
for $1 \leq k \leq k_{\text{max}}-1$.  For the latter of these, we
simply store the diagonals of the full Galerking coarsening of
$M_1^TM_1$ on all levels.  For the latter, we compute only the
diagonals of $A_k^TA_k$ by computing the sum of squares of entries in
each column of $A_k$.


While classical AMG applied to \eqref{eq:normal} would typically use a
Gauss-Seidel relaxation scheme, this is not practical here, since it
would require knowledge of $A^TA$ and its Galerkin coarse-grid
representations.  Instead, we make use of a weighted Jacobi iteration,
implemented in order to avoid undue expense.  Matrix-vector products
with the normal equations on each level are computed in two stages, by
first multiplying a vector on that level, $x_k$, by $A_k$ and $M_k$ to
form $\left[\begin{smallmatrix}
A_kx_k \\ \lambda^{(i)}M_kx_k \end{smallmatrix}\right]$ and then
multiplying by their transposes to complete the matrix-vector product.
The resulting residuals can be scaled by the inverse of the diagonal
on each level by a simple vector scaling, forming
$\left(\text{diag}(A_k^TA_k)
+ \left(\lambda^{(i)}\right)^2\text{diag}(M_k^TM_k)\right)^{-1}r_k$
based on the known vectors representing $\text{diag}(A_k^TA_k)$ and
$\text{diag}(M_k^TM_k)$ and the parameter $\lambda^{(i)}$.  Within the
multigrid cycle, we choose to use no sweeps of pre-relaxation and
three sweeps of post-relaxation.  To effectively choose the relaxation
parameters, we use diagonally preconditioned CG as relaxation on each
level, instead of a stationary (or Chebyshev accelerated) weighted
Jacobi relaxation.  This aids in the definition of a multigrid method
that is suitable for use for all values of $\lambda^{(i)}$, since no
explicit tuning of relaxation parameters as the regularization weight
changes is needed, as is shown below to be an effective approach.  The
use of Krylov acceleration within multigrid relaxation schemes is not
new \cite{REBank_CCDouglas_1985a, BReps_etal_2010a}; in preliminary
work (not reported here), experiments with fixed relaxation weights
showed that tuning was necessary with changes in both $D^{(\ell)}$ and
$\lambda^{(i)}$, so that using Krylov acceleration is an attractive
alternative.

