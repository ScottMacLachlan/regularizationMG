Multigrid methods (see, for
example, \cite{WLBriggs_VEHenson_SFMcCormick_2000a, UTrottenberg_etal_2001a}) are optimal solvers for systems arising from the
discretization of elliptic PDEs. Geometric multigrid methods make use
of the following observation: When an iterative method like (weighted)
Jacobi or Gauss-Seidel is applied to a linear system
\[
  A x = b, A \in \mathbb{R}^{n \times n}, x, b \in \mathbb{R}^n,
\]
arising from the discretization of a simple elliptic PDE, the
reduction of the error is relatively slow and depends on the
discretization parameter, $h$. Nevertheless, the error $e = A^{-1} b -
x$ is smooth after only a few iterations of the iterative
method. Obviously, a smooth error can be represented well on a coarser
mesh. Using the definition of the residual $r = b - A x$ we can obtain
the error as the solution of $A e = r$. If a coarse representation
$A_c$ of $A$ and $r_c$ of $r$ is available, the system $A_c e_c = r_c$
can be solved instead and the solution is less expensive. The
restriction of $r$ to $r_c$ can be obtained by simple injection or by
more complicated schemes such as, e.g., full-weighting. The coarse-grid
operator, $A_c$, is obtained by using a rediscretization of the
continuous operator on the coarser mesh, or by using a variational
principle resulting in the Galerkin operator. The error, $e_c$, on the
coarse mesh is then interpolated to the fine one, yielding an
approximation $\tilde{e}$. Finally, the current approximate solution
can be updated as $x = x + \tilde{e}$. As the interpolation process
introduces high frequency error components in the approximate
solution, in most cases a couple of additional relaxation steps are
applied. The resulting method is called a
\emph{two-grid} method.

This idea can be applied recursively on level
$k$ to solve the coarse system, $A_{k+1} e_{k+1} = r_{k+1}$,
using $\gamma$ iterations of the described method on the subsequent
level $k+1$, resulting in a multigrid method. Given relaxation
schemes on level $k$ specified by update operators,
$\mathcal{S}_k$ and $\tilde{\mathcal{S}}_k$,
$k = 0,\dots,k_\text{max}$, number of pre- and post-relaxation steps, $\nu_1$ and $\nu_2$, and restriction and prolongation operators,
$R_k$ and $P_k$, $k = 1,\dots,k_{\text{max}-1}$, a generic
multigrid algorithm can be found in Algorithm~\ref{alg:mg}.

\begin{algorithm}
  \caption{Multigrid cycle $x_k = \mathcal{MG}_k(x_k,b_k)$}
  \label{alg:mg}
  \begin{algorithmic}
    \STATE{$x_k \leftarrow
      \mathcal{S}_k^{\nu_1}(x_k,b_k)$}
    \STATE{$r_k \leftarrow b_k - A_k
      x_k$}
    \STATE{$r_{k+1} \leftarrow R_k r_k$}
    \STATE{$e_{k+1} \leftarrow 0$}
    \IF{$k+1 = k_\text{max}$}
    \STATE{$e_{k_\text{max}} \leftarrow
      A_{k_\text{max}}^{-1} r_{k_\text{max}}$}
  \ELSE
%  \FOR{$j = 1,\dots,\xi$}
  \STATE{$e_{k+1} \leftarrow
    \mathcal{MG}_{k+1}(e_{k+1},r_{k+1})$}
%  \ENDFOR
  \ENDIF
  \STATE{$e_k \leftarrow P_k e_{k+1}$}
  \STATE{$x_k \leftarrow x_k +
    e_k$}
  \STATE{$x_k \leftarrow
    \tilde{\mathcal{S}}_k^{\nu_2}(x_k,b_k)$}
  \end{algorithmic}
\end{algorithm}

In case that the coarsening is not easily determined geometrically, an
algebraic multigrid method (AMG) can be
used \cite{ABrandt_SFMcCormick_JWRuge_1984a, JWRuge_KStuben_1987a,
KStuben_2001a}. For a given relaxation scheme, classical AMG
automatically selects coarse mesh points and the necessary restriction
and prolongation operators based on certain heuristics that are
typically derived based on M-matrix or similar properties. AMG for
symmetric problems usually chooses $R_k = P_k^T$, the coarse-grid
operator is chosen to be the Galerkin coarse-grid operator given by
$A_{k+1} = P_k^T A_k P_k$.  In what follows, we make use of components
of the classical (Ruge-St\"uben) AMG algorithm as described
in \cite{JWRuge_KStuben_1987a} and elsewhere, but within a somewhat
nonstandard setup and solve phase, which are adapted to the problem
setting at hand.  {\bf TODO: describe classical AMG?}

\subsection{AMG for the regularized linear systems}

As noted before the second matrix $L^T W^2 L$ in \eqref{eq:normal}
corresponds to a variable-coefficient diffusion operator on a
two-dimensional grid. This can be solved efficiently using algebraic multigrid.

The first matrix in \eqref{eq:normal} corresponds to the operator that
has to be inverted. Usually, this is an integral operator, that is not
treated properly by algebraic multigrid. As the effective ratio
between the first and the second matrix in the system to be solved
depends on $\lambda$ we base our solver choice on the size of lambda
relative to the size of the singular values of $K^T K$ and $L^T W^2
L$.

For fixed $W$, the problem posed in \eqref{eq:regularized} is considered in 3 regimes:
\begin{enumerate}
\item When $\lambda > C \frac{\sigma(K)}{\sigma{WL}}$, where
  $\sigma(K)$ is the largest singular value of $K$ and $\sigma{WL}$ is
  the largest singular value of $WL$.  In this regime, the
  regularization (diffusive) term is dominant.
\item When $C \frac{\sigma(K)}{\sigma{WL}} > \lambda > c
  \frac{\sigma(K)}{\sigma{WL}}$, for $\mathcal{O}(1)$ constants $c$
  and $C$, the two terms in \eqref{eq:regularized} are {\it in
    balance}.
\item When $\lambda < c \frac{\sigma(K)}{\sigma{WL}}$, the data term
  in \eqref{eq:regularized} is dominant.
\end{enumerate}

In the first two cases, the diffusive term is significant, so the use
of a mulitigrid method is viable. We construct a multigrid method for
the regularized problem \eqref{eq:regularized} by reducing the size of
our solution, only. In terms of the block least-squares formulation
\eqref{eq:block-ls} the coarse-grid correction corresponds to finding
a correction in the range of $P$ minimizing
\[
\min_{y_c}\left\| \left[\begin{array}{c} K \\ \lambda
      WL\end{array}\right](\hat{x}+Py_c) - \left[\begin{array}{c} b \\ 0 \end{array}\right]\right\|^2.
\]
Due to the equivalence of the least-squares problem and the solution
of the normal equations \eqref{eq:normal} the coarse-grid solution $y_c$ is then characterized as the solution to
\[
\left(P^TK^TKP + \lambda^2P^TL^TW^2LP\right)y_c =
P^T\left[\begin{array}{c} K \\ \lambda WL\end{array}\right]^T
\left(\left[\begin{array}{c} b \\ 0 \end{array}\right] - \left[\begin{array}{c} K \\ \lambda
      WL\end{array}\right]\hat{x}\right),
\]
where the right-hand side is the restriction of the residual in the
normal equations associated with $\hat{x}$.

The equivalence of the formulations allows to design a multigrid
algorithms for the normal equations \eqref{eq:normal}, and apply it
directly to the sparse matrices in \eqref{eq:regularized}. As $K^TK$
can get dense, this results in a reduction of compute time, further
efficient implementations of the application of $K$, e.g., using fast
transformations, can be used directly.

To obtain $P$, only the matrix $A = L^T W^2 L$ is considered. We apply a
standard AMG setup phase to $A$ to create a hierarchy of interpolation
operators, $P$, and coarse-grid operators, $A_c = P^TAP$ (using
Galerkin coarsening).  From these interpolation operators, we also
create the \textit{one-sided} coarse-grid operators $K_c = KP$ and $(WL)_c =
WLP$, at all levels in the multigrid hierarchy. All that remains is to
specify the multigrid relaxation scheme and cycling parameters.  In
all that follows, we consider only multigrid V-cycles, as W-cycles (or
F-cycles) do not appear to be beneficial.

In the diffusion-dominated regime, the solution of
\eqref{eq:regularized} is primarily determined by the diffusive term
and, as such, we propose a multigrid smoother that is appropriate for
this term.  While classical AMG applied to \eqref{eq:normal} would
typically use a Gauss-Seidel relaxation scheme, this is not
appropriate here, since it would require knowledge of the (dense)
lower-triangular part of $K^TK$.  Instead, we use a red-black-ordered
Jacobi iteration, which allows us to make use of efficient
matrix-vector products with $K$ and $WL$ and their transposes.
Following reduction-based multigrid ideas \cite{}, we use only a
single sweep of pre-relaxation, in a CF ordering, first computing
updates to the coarse-grid points, then to those points on the fine
grid that are not directly represented on the coarse grid.  For each
of these sub-sweeps, we compute a full residual of the rectangular
form of the system,
\[
\hat{r} = \left[\begin{array}{c} b \\ 0 \end{array}\right] -
\left[\begin{array}{c} K \\ \lambda WL\end{array}\right]\hat{x}
\]
but then compute a Jacobi-like update at only the points, $j$, under
consideration, as
\[
\hat{x}_j + {\left(\left[\begin{array}{cc} K^T & \lambda
      L^TW\end{array}\right]\hat{r}\right)_j}/{ \left(K^TK +
  \lambda^2L^TW^2L\right)_{jj}}.
\]
We note that this requires computing only part of the matrix-vector
product with $\left[\begin{array}{cc} K^T & \lambda
    L^TW\end{array}\right]$ and the diagonal entries of $K^TK +
\lambda^2L^TW^2L$.  With this approach, a CF sweep of relaxation costs
the same as 3/2 matrix-vector products with the normal equations (when
done implicitly using the sparse matrices $K$ and $WL$).

Numerical experiments below show that this is an efficient relaxation
scheme for the problem, leading to good multigrid convergence, only
for sufficiently large $\lambda$.  In the second case, when the two
terms are more in balance, the multigrid approach can still be
effective, but the CF relaxation scheme is no longer appropriate,
since the diffusion term is not dominant.  Here, to better reflect the
impact of $K$ on the linear system, we use the CG algorithm as a
relaxation scheme, taking five steps of CG on the normal equations
(with no preconditioner).  Probably need some references here about
using CG as a smoother - Wim's papers on Helmholtz + GMRES, Randy Bank
paper, Xuejun Xu cascadic MG paper???...

We note that the interpolation and coarse-grid operators used above do
not depend on $\lambda$ and, thus, can be formed once for each choice
of $W$ and reused for all values of $\lambda$ for which a multigrid
iteration is needed.

When $\lambda$ is small compared to $\frac{\sigma(K)}{\sigma(WL)}$,
then the problem is essentially equivalent to the unregularized
problem $min_x \|Kx-b\|^2$. As the outer (nonlinear) iteration is
aiming to choose $W$ such that both terms are in balance, this case is
not the focus of this work. Nevertheless, to drive the nonlinear
iteration a sufficiently accurate solution is required for small
$\lambda$, as well. As this case occurs mainly early in the outer
iterations, the accuracy does not have to be very high, though. To
obtain a reasonable good approximation we use CG without
preconditioner on the normal equations here, allowing up to 50
iterations. Using the solution of the next-largest $\lambda$ value as
the initial guess yields very little improvement, and the improvement
to be gained decreases as $\lambda$ does.

% {\bf Ask Misha if there's something more reasonable to be done in
% the case of smallest values of $\lambda$.}
