Multigrid methods (see, for
example, \cite{WLBriggs_VEHenson_SFMcCormick_2000a, UTrottenberg_etal_2001a}) are optimal solvers for systems arising from the
discretization of elliptic PDEs. Geometric multigrid methods make use
of the following observation: When an iterative method like (weighted)
Jacobi or Gauss-Seidel is applied to a linear system
\[
  K x = b, K \in \mathbb{R}^{n \times n}, x, b \in \mathbb{R}^n,
\]
arising from the discretization of a simple elliptic PDE, the
reduction of the error is relatively slow and depends on the
discretization parameter, $h$. Nevertheless, the error $e = K^{-1} b -
x$ is smooth after only a few iterations of the iterative
method. Obviously, a smooth error can be represented well on a coarser
mesh. Using the definition of the residual $r = b - K x$ we can obtain
the error as the solution of $K e = r$. If coarse representations,
$K_c$ of $K$ and $r_c$ of $r$, are available, then the system $K_c e_c = r_c$
can be solved instead and the solution is less expensive. The
restriction of $r$ to $r_c$ can be obtained by simple injection or by
more complicated schemes such as, e.g., full-weighting. The coarse-grid
operator, $K_c$, can be obtained using a rediscretization of the
continuous operator on the coarser mesh, or by using a variational
principle resulting in the Galerkin operator. The error, $e_c$, on the
coarse mesh is then interpolated to the fine one, yielding an
approximation $\tilde{e}$. Finally, the current approximate solution
can be updated as $x = x + \tilde{e}$. As the interpolation process
introduces high frequency error components in the approximate
solution, in most cases a couple of additional relaxation steps are
applied. The resulting method is called a
\emph{two-grid} method.

This idea can be applied recursively on each level, with level
$k$ approximating the solution of the coarser system, $K_{k+1} e_{k+1} = r_{k+1}$,
using $\gamma$ iterations of the described method on
level $k+1$, resulting in a multigrid method. Given relaxation
schemes on level $k$ specified by update operators,
$\mathcal{S}_k$ and $\tilde{\mathcal{S}}_k$,
$k = 0,\dots,k_\text{max}$, number of pre- and post-relaxation steps, $\nu_1$ and $\nu_2$, and restriction and prolongation operators,
$R_k$ and $P_k$, $k = 1,\dots,k_{\text{max}-1}$, a generic
multigrid algorithm can be found in Algorithm~\ref{alg:mg}.

\begin{algorithm}
  \caption{Multigrid cycle $x_k = \mathcal{MG}_k(x_k,b_k)$}
  \label{alg:mg}
  \begin{algorithmic}
    \STATE{$x_k \leftarrow
      \mathcal{S}_k^{\nu_1}(x_k,b_k)$}
    \STATE{$r_k \leftarrow b_k - K_k
      x_k$}
    \STATE{$r_{k+1} \leftarrow R_k r_k$}
    \STATE{$e_{k+1} \leftarrow 0$}
    \IF{$k+1 = k_\text{max}$}
    \STATE{$e_{k_\text{max}} \leftarrow
      K_{k_\text{max}}^{-1} r_{k_\text{max}}$}
  \ELSE
%  \FOR{$j = 1,\dots,\xi$}
  \STATE{For $i=1,\ldots,\gamma$, $e_{k+1} \leftarrow
    \mathcal{MG}_{k+1}(e_{k+1},r_{k+1})$}
%  \ENDFOR
  \ENDIF
  \STATE{$e_k \leftarrow P_k e_{k+1}$}
  \STATE{$x_k \leftarrow x_k +
    e_k$}
  \STATE{$x_k \leftarrow
    \tilde{\mathcal{S}}_k^{\nu_2}(x_k,b_k)$}
  \end{algorithmic}
\end{algorithm}

In case that the coarsening is not easily determined geometrically, an
algebraic multigrid method (AMG) can be
used \cite{ABrandt_SFMcCormick_JWRuge_1984a, JWRuge_KStuben_1987a,
KStuben_2001a}. For a given relaxation scheme, classical AMG
automatically selects coarse mesh points and the necessary restriction
and prolongation operators based on certain heuristics that are
typically derived based on M-matrix or similar properties. AMG for
symmetric problems usually chooses $R_k = P_k^T$, the coarse-grid
operator is chosen to be the Galerkin coarse-grid operator given by
$K_{k+1} = P_k^T K_k P_k$.  Once this ``setup phase'' is complete, the
AMG solution phase proceeds exactly as in the geometric MG setting, as
described in Algorithm \ref{alg:mg}.

In what follows, we make use of components of the classical
(Ruge-St\"uben) AMG algorithm as described
in \cite{JWRuge_KStuben_1987a} and elsewhere, but within a somewhat
nonstandard setup and solve phase, which are adapted to the problem
setting at hand.  Given a matrix, $K$, the coarse-grid is selected
using a two-pass algorithm that partitions the degrees of freedom in
$K$ into two disjoint sets, denoted as C and F.  First, a maximal
independent set of the graph of the so-called strong connections in
matrix $K$ is formed.  This amounts to first filtering the matrix $K$
to remove entries where
\[
-k_{ij} \leq \theta \max_{m\neq i} \{-k_{im}\},
\]
where we take $\theta = 0.25$, as is commonly done.  The maximal
independent set is then formed by a greedy algorithm that sequentially
identifies the unsorted point with largest measure (initialized as
number of strong connections), marks this as a C-point, then marks its
as-yet unsorted strong neighbours as F-points and increments the
measure of the unsorted strong neighbours of the F-points to make
these more attractive for subsequent selection as C-points.  Given
this tentative partitioning, a second-pass is performed, where
additional points are moved from the F-set to the C-set, in order to
satisfy heuristic measures that indicate an effective interpolation
operator can be found.  Once the second pass has finalized the
partitioning, the interpolation operator, $P$, is formed.  For each
point in C, we assign a coarse-grid index to the point, and
interpolate directly from the coarse-grid index to its corresponding
point in C.  For interpolation to points in F, we make the assumption
that we are interpolating errors that vary smoothly between strongly
connected points, leading to an interpolation formula for each $i\in
F$ of $e_i = \sum_{j\in C_i} w_{ij}e_j$, where $C_i$ is the set of
points in C that are strongly connected to $i$.  The interpolation
weights are given by
\[
w_{ij} = - \frac{k_{ij} + \displaystyle\sum_{m \in
    F_i^s}\frac{k_{im}k_{mj}}{\sum_{l \in C_i} k_{ml}}}{k_{ii} +
    \displaystyle\sum_{m \in F_i^w} k_{im}},
\]
where the set $F_i^s$ is the set of points in F that are strongly
connected to $i$, while the set $F_i^w$ is defined so that all points
adjacent to $i$ are contained in the disjoint union $C_i \cup
F_i^s \cup F_i^w$.

\subsection{AMG for the regularized linear systems}

To clarify notation, we now rewrite \eqref{eq:sequence} to explictly
introduce separate indices for the regularization parameter and outer
iteration, defining $x^{(i,\ell)}$ as the vector that minimizes
\[
\| A x - b \|_2^2 + \left(\lambda^{(i)}\right)^2 \| M^{(\ell)} x \|_2^2.
\]
Notice that an equivalent formulation for $x^{(i,\ell)}$ is as the
minimizer of
\begin{equation}
\label{eq:block-ls}
\left\| \left[\begin{array}{c} A \\ \lambda^{(i)}
                           M^{(\ell)}\end{array}\right]x - \left[\begin{array}{c} b \\ 0 \end{array}\right]\right\|^2,
\end{equation}
for which the normal equations are readily given by
\begin{equation}
\label{eq:normal}
\left(A^TA + \left(\lambda^{(i)}\right)^2 \left(M^{(\ell)}\right)^TM^{(\ell)}\right)x^{(i,\ell)} = A^Tb.
\end{equation}
The first matrix in \eqref{eq:normal} corresponds to the ``data'' term
from the inverse problem, which is often an integral operator or
non-differential operator that is difficult to treat directly using
AMG due to its spectral properties and expected large number of
nonzero entries per row.  In contrast, the second matrix
in \eqref{eq:normal},
\[
\left(M^{(\ell)}\right)^TM^{(\ell)} = L^T\left(D^{(\ell)}\right)^2L,
\]
represents a variable-coefficient diffusion operator, for which AMG
techniques are expected to provide quite effective and efficient
solvers.

As a first observation, we note that for large-enough values of
$\lambda^{(i)}$, the system will be dominated by its second
component.  This motivates the design of an AMG-based algorithm to
solve \eqref{eq:normal}.  However, directly applying
AMG to $A^TA + \left(\lambda^{(i)}\right)^2
L^T\left(D^{(\ell)}\right)^2L$ is unlikely to be effective, due to the cost of applying the underlying graph
algorithms to the much denser matrix coming from $A^TA$.  Here, we
propose an alternative approach, that takes advantage of the
better sparsity of the rectangular operator in \eqref{eq:block-ls}
over the normal operator in \eqref{eq:normal}.

% SM stopped editing here 4/11/2020

The coarse-grid correction algorithm arises from applying classical
AMG to the ``regularization'' term in \eqref{eq:normal},
$L^T\left(W^{(i)}\right)^2L$, to determine an interpolation operator,
$P$.  Note that, on the finest grid, this term corresponds to a
discretized differential operator of the form for which AMG is
well-known to be effective, so no modifications to the classical AMG
heuristics are needed.  We apply a classical strength-of-connection
algorithm (with parameter $\theta = 0.25$), and the two-pass
Ruge-St\"uben coarsening algorithm as described
in \cite{JWRuge_KStuben_1987a}.  In addition to the Galerkin
coarse-grid operator, $P^TL^T\left(W^{(i)}\right)^2LP$, we also
compute the ``one-sided'' coarsening of the data matrix, $KP$.  Note
that this gives the necessary information to compute a matrix-vector
product with the full Galerkin coarse-grid operator, writing $P^TK^TKP
= (KP)^T(KP)$, without the (possibly significant) expense of forming
and storing $K^TK$.  Note, also, the important connection between such
a one-sided difference and the optimal coarse-grid correction from the
least-squares formulation, which would be to solve
\[
\min_{y_c}\left\| \left[\begin{array}{c} K \\ 
      \lambda^{(\ell)}W^{(i)}L\end{array}\right](\hat{x}+Py_c) - \left[\begin{array}{c} b \\ 0 \end{array}\right]\right\|^2,
\]
for a given approximation, $\hat{x}$, to $x^{(i,\ell)}$.  Coarser
levels in the multigrid hierarchy are defined recursively, applying
standard AMG to determine a coarse grid and interpolation operator for
$P^TL^T\left(W^{(i)}\right)^2LP$ and continuing the one-sided
coarsening of $KP$.  We note that the interpolation and coarse-grid
operators do not depend on $\lambda^{(\ell)}$ and, thus, can be formed
once for each outer iteration (choice of $W^{(i)}$) and reused for all
values of $\lambda^{(\ell)}$ for which a multigrid iteration is
needed.  To complete the specification of the multigrid algorithm, all
that remains is to specify the multigrid relaxation scheme and cycling
parameters.  In all that follows, we consider only multigrid V-cycles,
as W-cycles (or F-cycles) do not appear to be needed or beneficial.
{\bf TODO: Should I discuss that we store only $WLP$ here and not
$P^TL^TW^2LP$?  We don't actually make use of this, but that's how we
do it...}

While classical AMG applied to \eqref{eq:normal} would typically use a
Gauss-Seidel relaxation scheme, this is not practical here, since it
would require knowledge of $K^TK$ and its Galerkin coarse-grid
representations.  Instead, we make use of a weighted Jacobi iteration.
Even this, however, has to be implemented carefully, in order to avoid
undue expense.  To do this, on each level of the multigrid hierarchy,
we store two additional vectors, corresponding to the entries on the
diagonals of matrices $(KP)^T(KP)$ and
$P^TL^T\left(W^{(i)}\right)^2LP$, where $P$ represents the composite
interpolation operator from the current level to the finest grid.  The
latter of these is naturally computed explicitly in the AMG setup
stage and is simply stored for convenience.  The first of these,
however, represents an additional calculation.  To do this
efficiently, we note that the diagonal entries of $(KP)^T(KP)$ are
equal to the column-wise sum of squares of entries in $KP$.  Since
$KP$ is formed explicitly in our setup stage, computing the diagonal
entries of $(KP)^T(KP)$ is also straightforward.  With these two
entries, for any value of $\lambda^{(\ell)}$, we can scale any vector
by the diagonal of $(KP)^T(KP)
+ \left(\lambda^{(\ell)}\right)^2P^TL^T\left(W^{(i)}\right)^2LP$ with
a simple vector operation.

Within the multigrid cycle, we choose to use no sweeps of
pre-relaxation and three sweeps of post-relaxation.  To effectively
choose the relaxation parameters, we make use of diagonally
preconditioned CG as relaxation on each level, with the diagonal
specified by the vectors discussed above.  {\bf TODO: Probably need
some references here about using CG as a smoother - Wim's papers on
Helmholtz + GMRES, Randy Bank paper, Xuejun Xu cascadic MG
paper???...}  In the numerical results below, this is shown to be an
effective cycling scheme.  Heuristically, for each value of
$\lambda^{(\ell)}$, there are two effects that complicate the choice
of a fixed relaxation weight.  Clearly, the relative scaling between
the two terms changes dramatically as $\lambda^{(\ell)}$ changes, just
from the possibilities of having large or small values of the
regularization parameter (that are squared in the normal operator).
Secondly, there is the expected inversion of scaling, with
eigenvectors associated with large eigenvalues of $K^TK$ typically
corresponding to small eigenvalues of the regularization term and
vice-versa.  The (nonlinear) interaction between the spectral
behaviour of the two terms makes it quite difficult to fix relaxation
parameters by hand and, so, using a Krylov wrapper to effectively
choose weights is quite appealing.

As a consequence of the non-stationary relaxation procedure, we use
Flexible GMRES (FGMRES) \cite{YSaad_2003a} as the outer wrapper for
solving each linear system of the form \eqref{eq:normal}.  A stopping
tolerance requiring the $\ell_2$ norm of the residual to be less than
a factor of $10^{-4}$ times the norm of $K^Tb$ is used.  For each
outermost iteration, we cycle through the chosen values of
$\lambda^{(\ell)}$ from largest to smallest, using the solution of the
next-largest regularization parameter as the initial guess for each
system.  For the largest value of $\lambda^{(\ell)}$, we use the
solution for the chosen $\lambda_{i-1}$ from the previous iteration as
the initial guess, with a zero initial guess used for the first outer
iteration.  Note that, since the solution is expected to vary
continuously with $\lambda^{(\ell)}$, this generally provides very
good initial guesses, at least as we vary the regularization parameter
within each outer iteration, motivating the use of a stopping
tolerance that is not relative to the initial residual.

