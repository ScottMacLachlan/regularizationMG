Multigrid methods (see, for
example, \cite{WLBriggs_VEHenson_SFMcCormick_2000a, UTrottenberg_etal_2001a}) are optimal solvers for systems arising from the
discretization of elliptic PDEs. Geometric multigrid methods make use
of the following observation: When an iterative method like (weighted)
Jacobi or Gauss-Seidel is applied to a linear system
\[
  A x = b, A \in \mathbb{R}^{n \times n}, x, b \in \mathbb{R}^n,
\]
arising from the discretization of a simple elliptic PDE, the
reduction of the error is relatively slow and depends on the
discretization parameter, $h$. Nevertheless, the error $e = A^{-1} b -
x$ is smooth after only a few iterations of the iterative
method. Obviously, a smooth error can be represented well on a coarser
mesh. Using the definition of the residual $r = b - A x$ we can obtain
the error as the solution of $A e = r$. If a coarse representation
$A_c$ of $A$ and $r_c$ of $r$ is available, the system $A_c e_c = r_c$
can be solved instead and the solution is less expensive. The
restriction of $r$ to $r_c$ can be obtained by simple injection or by
more complicated schemes such as, e.g., full-weighting. The coarse-grid
operator, $A_c$, is obtained by using a rediscretization of the
continuous operator on the coarser mesh, or by using a variational
principle resulting in the Galerkin operator. The error, $e_c$, on the
coarse mesh is then interpolated to the fine one, yielding an
approximation $\tilde{e}$. Finally, the current approximate solution
can be updated as $x = x + \tilde{e}$. As the interpolation process
introduces high frequency error components in the approximate
solution, in most cases a couple of additional relaxation steps are
applied. The resulting method is called a
\emph{two-grid} method.

This idea can be applied recursively on level
$k$ to solve the coarse system, $A_{k+1} e_{k+1} = r_{k+1}$,
using $\gamma$ iterations of the described method on the subsequent
level $k+1$, resulting in a multigrid method. Given relaxation
schemes on level $k$ specified by update operators,
$\mathcal{S}_k$ and $\tilde{\mathcal{S}}_k$,
$k = 0,\dots,k_\text{max}$, number of pre- and post-relaxation steps, $\nu_1$ and $\nu_2$, and restriction and prolongation operators,
$R_k$ and $P_k$, $k = 1,\dots,k_{\text{max}-1}$, a generic
multigrid algorithm can be found in Algorithm~\ref{alg:mg}.

\begin{algorithm}
  \caption{Multigrid cycle $x_k = \mathcal{MG}_k(x_k,b_k)$}
  \label{alg:mg}
  \begin{algorithmic}
    \STATE{$x_k \leftarrow
      \mathcal{S}_k^{\nu_1}(x_k,b_k)$}
    \STATE{$r_k \leftarrow b_k - A_k
      x_k$}
    \STATE{$r_{k+1} \leftarrow R_k r_k$}
    \STATE{$e_{k+1} \leftarrow 0$}
    \IF{$k+1 = k_\text{max}$}
    \STATE{$e_{k_\text{max}} \leftarrow
      A_{k_\text{max}}^{-1} r_{k_\text{max}}$}
  \ELSE
%  \FOR{$j = 1,\dots,\xi$}
  \STATE{$e_{k+1} \leftarrow
    \mathcal{MG}_{k+1}(e_{k+1},r_{k+1})$}
%  \ENDFOR
  \ENDIF
  \STATE{$e_k \leftarrow P_k e_{k+1}$}
  \STATE{$x_k \leftarrow x_k +
    e_k$}
  \STATE{$x_k \leftarrow
    \tilde{\mathcal{S}}_k^{\nu_2}(x_k,b_k)$}
  \end{algorithmic}
\end{algorithm}

In case that the coarsening is not easily determined geometrically, an
algebraic multigrid method (AMG) can be
used \cite{ABrandt_SFMcCormick_JWRuge_1984a, JWRuge_KStuben_1987a,
KStuben_2001a}. For a given relaxation scheme, classical AMG
automatically selects coarse mesh points and the necessary restriction
and prolongation operators based on certain heuristics that are
typically derived based on M-matrix or similar properties. AMG for
symmetric problems usually chooses $R_k = P_k^T$, the coarse-grid
operator is chosen to be the Galerkin coarse-grid operator given by
$A_{k+1} = P_k^T A_k P_k$.  In what follows, we make use of components
of the classical (Ruge-St\"uben) AMG algorithm as described
in \cite{JWRuge_KStuben_1987a} and elsewhere, but within a somewhat
nonstandard setup and solve phase, which are adapted to the problem
setting at hand.  {\bf TODO: describe classical AMG?}

\subsection{AMG for the regularized linear systems}

Considering Equation \eqref{eq:defining_xil}, we notice that an
equivalent formulation for $x^{(i,\ell)}$ can be written as
\begin{equation}
\label{eq:block-ls}
\min_{x^{(i,\ell)}}\left\| \left[\begin{array}{c} K \\ \lambda^{(\ell)}
                           W^{(i)}L\end{array}\right]x^{(i,\ell)} - \left[\begin{array}{c} b \\ 0 \end{array}\right]\right\|^2,
\end{equation}
for which the normal equations are readily given by
\begin{equation}
\label{eq:normal}
\left(K^TK + \left(\lambda^{(\ell)}\right)^2 L^T\left(W^{(i)}\right)^2L\right)x^{(i,\ell)} = K^Tb.
\end{equation}
The first matrix in \eqref{eq:normal} corresponds to the ``data'' term
from the inverse problem, which is often an integral operator or
non-differential operator that is difficult to treat directly using
AMG due to its spectral properties and expected large number of
nonzero entries per row.  In contrast, the second matrix
in \eqref{eq:normal} represents a variable-coefficient diffusion
operator, for which AMG techniques are expected to provide quite
effective and efficient solvers.

As a first observation, we note that for large-enough values of
$\lambda^{(\ell)}$, the system will be dominated by its second
component.  This motivates the design of an AMG-based algorithm to
solve \eqref{eq:normal}.  As noted above, however, directly applying
AMG to $K^TK + \left(\lambda^{(\ell)}\right)^2
L^T\left(W^{(i)}\right)^2L$ is unlikely to provide an effective
preconditioner, due to the cost of applying the underlying graph
algorithms to the much denser matrix coming from $K^TK$.  Here, we
propose a ``hybrid'' multigrid approach, that takes advantage of the
better sparsity of the rectangular sparsity in \eqref{eq:block-ls}
over the normal operator in \eqref{eq:normal}.

The coarse-grid correction algorithm arises from applying classical
AMG to the ``regularization'' term in \eqref{eq:normal},
$L^T\left(W^{(i)}\right)^2L$, to determine an interpolation operator,
$P$.  Note that, on the finest grid, this term corresponds to a
discretized differential operator of the form for which AMG is
well-known to be effective, so no modifications to the classical AMG
heuristics are needed.  We apply a classical strength-of-connection
algorithm (with parameter $\theta = 0.25$), and the two-pass
Ruge-St\"uben coarsening algorithm as described
in \cite{JWRuge_KStuben_1987a}.  In addition to the Galerkin
coarse-grid operator, $P^TL^T\left(W^{(i)}\right)^2LP$, we also
compute the ``one-sided'' coarsening of the data matrix, $KP$.  Note
that this gives the necessary information to compute a matrix-vector
product with the full Galerkin coarse-grid operator, writing $P^TK^TKP
= (KP)^T(KP)$, without the (possibly significant) expense of forming
and storing $K^TK$.  Note, also, the important connection between such
a one-sided difference and the optimal coarse-grid correction from the
least-squares formulation, which would be to solve
\[
\min_{y_c}\left\| \left[\begin{array}{c} K \\ 
      \lambda^{(\ell)}W^{(i)}L\end{array}\right](\hat{x}+Py_c) - \left[\begin{array}{c} b \\ 0 \end{array}\right]\right\|^2,
\]
for a given approximation, $\hat{x}$, to $x^{(i,\ell)}$.  Coarser
levels in the multigrid hierarchy are defined recursively, applying
standard AMG to determine a coarse grid and interpolation operator for
$P^TL^T\left(W^{(i)}\right)^2LP$ and continuing the one-sided
coarsening of $KP$.  We note that the interpolation and coarse-grid
operators do not depend on $\lambda^{(\ell)}$ and, thus, can be formed
once for each outer iteration (choice of $W^{(i)}$) and reused for all
values of $\lambda^{(\ell)}$ for which a multigrid iteration is
needed.  To complete the specification of the multigrid algorithm, all
that remains is to specify the multigrid relaxation scheme and cycling
parameters.  In all that follows, we consider only multigrid V-cycles,
as W-cycles (or F-cycles) do not appear to be needed or beneficial.
{\bf TODO: Should I discuss that we store only $WLP$ here and not
$P^TL^TW^2LP$?  We don't actually make use of this, but that's how we
do it...}

While classical AMG applied to \eqref{eq:normal} would typically use a
Gauss-Seidel relaxation scheme, this is not practical here, since it
would require knowledge of $K^TK$ and its Galerkin coarse-grid
representations.  Instead, we make use of a weighted Jacobi iteration.
Even this, however, has to be implemented carefully, in order to avoid
undue expense.  To do this, on each level of the multigrid hierarchy,
we store two additional vectors, corresponding to the entries on the
diagonals of matrices $(KP)^T(KP)$ and
$P^TL^T\left(W^{(i)}\right)^2LP$, where $P$ represents the composite
interpolation operator from the current level to the finest grid.  The
latter of these is naturally computed explicitly in the AMG setup
stage and is simply stored for convenience.  The first of these,
however, represents an additional calculation.  To do this
efficiently, we note that the diagonal entries of $(KP)^T(KP)$ are
equal to the column-wise sum of squares of entries in $KP$.  Since
$KP$ is formed explicitly in our setup stage, computing the diagonal
entries of $(KP)^T(KP)$ is also straightforward.  With these two
entries, for any value of $\lambda^{(\ell)}$, we can scale any vector
by the diagonal of $(KP)^T(KP)
+ \left(\lambda^{(\ell)}\right)^2P^TL^T\left(W^{(i)}\right)^2LP$ with
a simple vector operation.

Within the multigrid cycle, we choose to use no sweeps of
pre-relaxation and three sweeps of post-relaxation.  To effectively
choose the relaxation parameters, we make use of diagonally
preconditioned CG as relaxation on each level, with the diagonal
specified by the vectors discussed above.  {\bf TODO: Probably need
some references here about using CG as a smoother - Wim's papers on
Helmholtz + GMRES, Randy Bank paper, Xuejun Xu cascadic MG
paper???...}  In the numerical results below, this is shown to be an
effective cycling scheme.  Heuristically, for each value of
$\lambda^{(\ell)}$, there are two effects that complicate the choice
of a fixed relaxation weight.  Clearly, the relative scaling between
the two terms changes dramatically as $\lambda^{(\ell)}$ changes, just
from the possibilities of having large or small values of the
regularization parameter (that are squared in the normal operator).
Secondly, there is the expected inversion of scaling, with
eigenvectors associated with large eigenvalues of $K^TK$ typically
corresponding to small eigenvalues of the regularization term and
vice-versa.  The (nonlinear) interaction between the spectral
behaviour of the two terms makes it quite difficult to fix relaxation
parameters by hand and, so, using a Krylov wrapper to effectively
choose weights is quite appealing.

As a consequence of the non-stationary relaxation procedure, we use
Flexible GMRES (FGMRES) \cite{YSaad_2003a} as the outer wrapper for
solving each linear system of the form \eqref{eq:normal}.  A stopping
tolerance requiring the $\ell_2$ norm of the residual to be less than
a factor of $10^{-4}$ times the norm of $K^Tb$ is used.  For each
outermost iteration, we cycle through the chosen values of
$\lambda^{(\ell)}$ from largest to smallest, using the solution of the
next-largest regularization parameter as the initial guess for each
system.  For the largest value of $\lambda^{(\ell)}$, we use the
solution for the chosen $\lambda_{i-1}$ from the previous iteration as
the initial guess, with a zero initial guess used for the first outer
iteration.  Note that, since the solution is expected to vary
continuously with $\lambda^{(\ell)}$, this generally provides very
good initial guesses, at least as we vary the regularization parameter
within each outer iteration, motivating the use of a stopping
tolerance that is not relative to the initial residual.

